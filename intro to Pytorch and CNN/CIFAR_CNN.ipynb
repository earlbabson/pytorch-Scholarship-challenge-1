{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CIFAR_CNN.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"u729Ok1sOsz5","colab_type":"code","colab":{}},"cell_type":"code","source":["!git clone https://gist.github.com/f7b7c7758a46da49f84bc68b47997d69.git"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ecusg7GWmLxD","colab_type":"code","colab":{}},"cell_type":"code","source":["cd f7b7c7758a46da49f84bc68b47997d69"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cTwLAJhvmZEV","colab_type":"code","colab":{}},"cell_type":"code","source":["!bash pytorch041_cuda92_colab.sh"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RBnam09lmeF1","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","from torchvision import datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data.sampler import SubsetRandomSampler"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ubONbrmJn485","colab_type":"code","colab":{}},"cell_type":"code","source":["# number of subprocesses to use for data loading\n","num_workers = 0\n","# how many samples per batch to load\n","batch_size = 20\n","# percentage of training set to use as validation\n","valid_size = 0.2\n","\n","# convert data to a normalized torch.FloatTensor\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    ])\n","\n","# choose the training and test datasets\n","train_data = datasets.CIFAR10('data', train=True,\n","                              download=True, transform=transform)\n","test_data = datasets.CIFAR10('data', train=False,\n","                             download=True, transform=transform)\n","\n","# obtain training indices that will be used for validation\n","num_train = len(train_data)\n","indices = list(range(num_train))\n","np.random.shuffle(indices)\n","split = int(np.floor(valid_size * num_train))\n","train_idx, valid_idx = indices[split:], indices[:split]\n","\n","# define samplers for obtaining training and validation batches\n","train_sampler = SubsetRandomSampler(train_idx)\n","valid_sampler = SubsetRandomSampler(valid_idx)\n","\n","# prepare data loaders (combine dataset and sampler)\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n","    sampler=train_sampler, num_workers=num_workers)\n","valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n","    sampler=valid_sampler, num_workers=num_workers)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n","    num_workers=num_workers)\n","\n","# specify the image classes\n","classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n","           'dog', 'frog', 'horse', 'ship', 'truck']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-rPi4FeVohgC","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = x.view(-1, 16 * 5 * 5)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","\n","# create a complete CNN\n","model = Net()\n","model.cuda()\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.5)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"brCJ09lqs5bd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":199},"outputId":"195a2fa9-48db-4142-fce2-e5306d0ab142","executionInfo":{"status":"ok","timestamp":1544474146667,"user_tz":480,"elapsed":176042,"user":{"displayName":"Ritul Patidar","photoUrl":"https://lh4.googleusercontent.com/-FN-NHp32ujY/AAAAAAAAAAI/AAAAAAAAAGg/NYELu2XQ7Lk/s64/photo.jpg","userId":"11865105037108873448"}}},"cell_type":"code","source":["n_epochs = 8 # you may increase this number to train a final model\n","\n","valid_loss_min = np.Inf # track change in validation loss\n","\n","for epoch in range(1, n_epochs+1):\n","\n","    # keep track of training and validation loss\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","    \n","   \n","    # train the model \n","   \n","    model.train()\n","    for data, target in train_loader:\n","       \n","        data, target = data.cuda(), target.cuda()\n","        # clear the gradients of all optimized variables\n","        optimizer.zero_grad()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        output = model(data)\n","        # calculate the batch loss\n","        loss = criterion(output, target)\n","        # backward pass: compute gradient of the loss with respect to model parameters\n","        loss.backward()\n","        # perform a single optimization step (parameter update)\n","        optimizer.step()\n","        # update training loss\n","        train_loss += loss.item()*data.size(0)\n","    \n","    # validate the model \n","   \n","    model.eval()\n","    for data, target in valid_loader:\n","        \n","        data, target = data.cuda(), target.cuda()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        output = model(data)\n","        # calculate the batch loss\n","        loss = criterion(output, target)\n","        # update average validation loss \n","        valid_loss += loss.item()*data.size(0)\n","    \n","    # calculate average losses\n","    train_loss = train_loss/len(train_loader.dataset)\n","    valid_loss = valid_loss/len(valid_loader.dataset)\n","        \n","    # print training/validation statistics \n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","        epoch, train_loss, valid_loss))\n","    \n","    # save model if validation loss has decreased\n","    if valid_loss <= valid_loss_min:\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","        valid_loss_min,\n","        valid_loss))\n","        torch.save(model.state_dict(), 'model_cifar.pt')\n","        valid_loss_min = valid_loss"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Epoch: 1 \tTraining Loss: 0.818229 \tValidation Loss: 0.259494\n","Validation loss decreased (inf --> 0.259494).  Saving model ...\n","Epoch: 2 \tTraining Loss: 0.811761 \tValidation Loss: 0.260804\n","Epoch: 3 \tTraining Loss: 0.796950 \tValidation Loss: 0.269050\n","Epoch: 4 \tTraining Loss: 0.782326 \tValidation Loss: 0.258993\n","Validation loss decreased (0.259494 --> 0.258993).  Saving model ...\n","Epoch: 5 \tTraining Loss: 0.772997 \tValidation Loss: 0.270458\n","Epoch: 6 \tTraining Loss: 0.764466 \tValidation Loss: 0.275785\n","Epoch: 7 \tTraining Loss: 0.760896 \tValidation Loss: 0.281264\n","Epoch: 8 \tTraining Loss: 0.764045 \tValidation Loss: 0.272477\n"],"name":"stdout"}]},{"metadata":{"id":"7GvIerWeteA0","colab_type":"code","colab":{}},"cell_type":"code","source":["model.load_state_dict(torch.load('model_cifar.pt'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QoPKTPQswF6C","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":272},"outputId":"b5d8a981-e0c9-4a66-9c39-e17b10cd6a31","executionInfo":{"status":"ok","timestamp":1544472933355,"user_tz":480,"elapsed":6717,"user":{"displayName":"Ritul Patidar","photoUrl":"https://lh4.googleusercontent.com/-FN-NHp32ujY/AAAAAAAAAAI/AAAAAAAAAGg/NYELu2XQ7Lk/s64/photo.jpg","userId":"11865105037108873448"}}},"cell_type":"code","source":["# track test loss\n","test_loss = 0.0\n","class_correct = list(0. for i in range(10))\n","class_total = list(0. for i in range(10))\n","\n","model.eval()\n","# iterate over test data\n","for data, target in test_loader:\n","    # move tensors to GPU if CUDA is available\n","    \n","    data, target = data.cuda(), target.cuda()\n","    # forward pass: compute predicted outputs by passing inputs to the model\n","    output = model(data)\n","    # calculate the batch loss\n","    loss = criterion(output, target)\n","    # update test loss \n","    test_loss += loss.item()*data.size(0)\n","    # convert output probabilities to predicted class\n","    _, pred = torch.max(output, 1)    \n","    # compare predictions to true label\n","    correct_tensor = pred.eq(target.data.view_as(pred))\n","    correct = np.squeeze(correct_tensor.cpu().numpy()) \n","    # calculate test accuracy for each object class\n","    for i in range(batch_size):\n","        label = target.data[i]\n","        class_correct[label] += correct[i].item()\n","        class_total[label] += 1\n","\n","# average test loss\n","test_loss = test_loss/len(test_loader.dataset)\n","print('Test Loss: {:.6f}\\n'.format(test_loss))\n","\n","for i in range(10):\n","    if class_total[i] > 0:\n","        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n","            classes[i], 100 * class_correct[i] / class_total[i],\n","            np.sum(class_correct[i]), np.sum(class_total[i])))\n","    else:\n","        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n","\n","print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n","    100. * np.sum(class_correct) / np.sum(class_total),\n","    np.sum(class_correct), np.sum(class_total)))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Test Loss: 1.061392\n","\n","Test Accuracy of airplane: 69% (690/1000)\n","Test Accuracy of automobile: 76% (760/1000)\n","Test Accuracy of  bird: 47% (479/1000)\n","Test Accuracy of   cat: 39% (396/1000)\n","Test Accuracy of  deer: 59% (592/1000)\n","Test Accuracy of   dog: 52% (529/1000)\n","Test Accuracy of  frog: 71% (719/1000)\n","Test Accuracy of horse: 75% (750/1000)\n","Test Accuracy of  ship: 79% (799/1000)\n","Test Accuracy of truck: 69% (693/1000)\n","\n","Test Accuracy (Overall): 64% (6407/10000)\n"],"name":"stdout"}]},{"metadata":{"id":"E9lYGy2QwI7K","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}