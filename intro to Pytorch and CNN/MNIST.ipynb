{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled4.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"sUqVrXGvw1s4","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch\n","from torchvision import datasets\n","import torchvision.transforms as transforms\n","import numpy as np"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-ms8PJ5TxK_h","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":126},"outputId":"36d52314-c7ec-49be-81b4-932727e67eb6","executionInfo":{"status":"ok","timestamp":1543265847259,"user_tz":480,"elapsed":2443,"user":{"displayName":"Ritul Patidar","photoUrl":"https://lh4.googleusercontent.com/-FN-NHp32ujY/AAAAAAAAAAI/AAAAAAAAAGg/NYELu2XQ7Lk/s64/photo.jpg","userId":"11865105037108873448"}}},"cell_type":"code","source":["\n","num_workers = 0\n","\n","batch_size = 20\n","\n","# convert data to torch.FloatTensor\n","transform = transforms.ToTensor()\n","\n","# choose the training and test datasets\n","train_data = datasets.MNIST(root='data', train=True,\n","                                   download=True, transform=transform)\n","test_data = datasets.MNIST(root='data', train=False,\n","                                  download=True, transform=transform)\n","\n","# prepare data loaders\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n","    num_workers=num_workers)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n","    num_workers=num_workers)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Processing...\n","Done!\n"],"name":"stdout"}]},{"metadata":{"id":"976pgH1Nz_3t","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# define the NN architecture\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # number of hidden nodes in each layer (512)\n","        hidden_1 = 512\n","        hidden_2 = 512\n","        # linear layer (784 -> hidden_1)\n","        self.fc1 = nn.Linear(28 * 28, hidden_1)\n","        # linear layer (n_hidden -> hidden_2)\n","        self.fc2 = nn.Linear(hidden_1, hidden_2)\n","        # linear layer (n_hidden -> 10)\n","        self.fc3 = nn.Linear(hidden_2, 10)\n","        # dropout layer (p=0.2)\n","        # dropout prevents overfitting of data\n","        self.dropout = nn.Dropout(0.2)\n","\n","    def forward(self, x):\n","        # flatten image input\n","        x = x.view(-1, 28 * 28)\n","        # add hidden layer, with relu activation function\n","        x = F.relu(self.fc1(x))\n","        # add dropout layer\n","        x = self.dropout(x)\n","        # add hidden layer, with relu activation function\n","        x = F.relu(self.fc2(x))\n","        # add dropout layer\n","        x = self.dropout(x)\n","        # add output layer\n","        x = self.fc3(x)\n","        return x\n","\n","# initialize the NN\n","model = Net()\n","\n","# specify loss function (categorical cross-entropy)\n","criterion = nn.CrossEntropyLoss()\n","\n","# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RpZa5mNa0GNz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":926},"outputId":"bb6666af-2354-4224-b3b7-127048297244","executionInfo":{"status":"ok","timestamp":1543266787210,"user_tz":480,"elapsed":861908,"user":{"displayName":"Ritul Patidar","photoUrl":"https://lh4.googleusercontent.com/-FN-NHp32ujY/AAAAAAAAAAI/AAAAAAAAAGg/NYELu2XQ7Lk/s64/photo.jpg","userId":"11865105037108873448"}}},"cell_type":"code","source":["# number of epochs to train the model\n","n_epochs = 50\n","\n","model.train() # prep model for training\n","\n","for epoch in range(n_epochs):\n","    # monitor training loss\n","    train_loss = 0.0\n","    \n","    ###################\n","    # train the model #\n","    ###################\n","    for data, target in train_loader:\n","        # clear the gradients of all optimized variables\n","        optimizer.zero_grad()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        output = model(data)\n","        # calculate the loss\n","        loss = criterion(output, target)\n","        # backward pass: compute gradient of the loss with respect to model parameters\n","        loss.backward()\n","        # perform a single optimization step (parameter update)\n","        optimizer.step()\n","        # update running training loss\n","        train_loss += loss.item()*data.size(0)\n","             \n","    # print training statistics \n","    # calculate average loss over an epoch\n","    train_loss = train_loss/len(train_loader.dataset)\n","\n","    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n","        epoch+1, \n","        train_loss\n","        ))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Epoch: 1 \tTraining Loss: 0.815939\n","Epoch: 2 \tTraining Loss: 0.320701\n","Epoch: 3 \tTraining Loss: 0.248348\n","Epoch: 4 \tTraining Loss: 0.201966\n","Epoch: 5 \tTraining Loss: 0.169304\n","Epoch: 6 \tTraining Loss: 0.146618\n","Epoch: 7 \tTraining Loss: 0.129873\n","Epoch: 8 \tTraining Loss: 0.115656\n","Epoch: 9 \tTraining Loss: 0.103717\n","Epoch: 10 \tTraining Loss: 0.094061\n","Epoch: 11 \tTraining Loss: 0.087395\n","Epoch: 12 \tTraining Loss: 0.080343\n","Epoch: 13 \tTraining Loss: 0.073384\n","Epoch: 14 \tTraining Loss: 0.069348\n","Epoch: 15 \tTraining Loss: 0.064759\n","Epoch: 16 \tTraining Loss: 0.058662\n","Epoch: 17 \tTraining Loss: 0.055572\n","Epoch: 18 \tTraining Loss: 0.052114\n","Epoch: 19 \tTraining Loss: 0.049507\n","Epoch: 20 \tTraining Loss: 0.045669\n","Epoch: 21 \tTraining Loss: 0.043737\n","Epoch: 22 \tTraining Loss: 0.041296\n","Epoch: 23 \tTraining Loss: 0.038904\n","Epoch: 24 \tTraining Loss: 0.036927\n","Epoch: 25 \tTraining Loss: 0.034867\n","Epoch: 26 \tTraining Loss: 0.032423\n","Epoch: 27 \tTraining Loss: 0.030553\n","Epoch: 28 \tTraining Loss: 0.029724\n","Epoch: 29 \tTraining Loss: 0.027872\n","Epoch: 30 \tTraining Loss: 0.027145\n","Epoch: 31 \tTraining Loss: 0.025649\n","Epoch: 32 \tTraining Loss: 0.025309\n","Epoch: 33 \tTraining Loss: 0.024221\n","Epoch: 34 \tTraining Loss: 0.021326\n","Epoch: 35 \tTraining Loss: 0.021730\n","Epoch: 36 \tTraining Loss: 0.021134\n","Epoch: 37 \tTraining Loss: 0.019892\n","Epoch: 38 \tTraining Loss: 0.019248\n","Epoch: 39 \tTraining Loss: 0.017698\n","Epoch: 40 \tTraining Loss: 0.016492\n","Epoch: 41 \tTraining Loss: 0.017006\n","Epoch: 42 \tTraining Loss: 0.016277\n","Epoch: 43 \tTraining Loss: 0.015353\n","Epoch: 44 \tTraining Loss: 0.015247\n","Epoch: 45 \tTraining Loss: 0.014334\n","Epoch: 46 \tTraining Loss: 0.013451\n","Epoch: 47 \tTraining Loss: 0.013216\n","Epoch: 48 \tTraining Loss: 0.012770\n","Epoch: 49 \tTraining Loss: 0.011996\n","Epoch: 50 \tTraining Loss: 0.011492\n"],"name":"stdout"}]},{"metadata":{"id":"3Kvxx86f0Tg4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":272},"outputId":"4672f20e-76a8-48a2-f240-eedae46891c6","executionInfo":{"status":"ok","timestamp":1543266796144,"user_tz":480,"elapsed":2914,"user":{"displayName":"Ritul Patidar","photoUrl":"https://lh4.googleusercontent.com/-FN-NHp32ujY/AAAAAAAAAAI/AAAAAAAAAGg/NYELu2XQ7Lk/s64/photo.jpg","userId":"11865105037108873448"}}},"cell_type":"code","source":["# initialize lists to monitor test loss and accuracy\n","test_loss = 0.0\n","class_correct = list(0. for i in range(10))\n","class_total = list(0. for i in range(10))\n","\n","model.eval() # prep model for evaluation\n","\n","for data, target in test_loader:\n","    # forward pass: compute predicted outputs by passing inputs to the model\n","    output = model(data)\n","    # calculate the loss\n","    loss = criterion(output, target)\n","    # update test loss \n","    test_loss += loss.item()*data.size(0)\n","    # convert output probabilities to predicted class\n","    _, pred = torch.max(output, 1)\n","    # compare predictions to true label\n","    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n","    # calculate test accuracy for each object class\n","    for i in range(batch_size):\n","        label = target.data[i]\n","        class_correct[label] += correct[i].item()\n","        class_total[label] += 1\n","\n","# calculate and print avg test loss\n","test_loss = test_loss/len(test_loader.dataset)\n","print('Test Loss: {:.6f}\\n'.format(test_loss))\n","\n","for i in range(10):\n","    if class_total[i] > 0:\n","        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n","            str(i), 100 * class_correct[i] / class_total[i],\n","            np.sum(class_correct[i]), np.sum(class_total[i])))\n","    else:\n","        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n","\n","print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n","    100. * np.sum(class_correct) / np.sum(class_total),\n","    np.sum(class_correct), np.sum(class_total)))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Test Loss: 0.054438\n","\n","Test Accuracy of     0: 99% (971/980)\n","Test Accuracy of     1: 99% (1128/1135)\n","Test Accuracy of     2: 98% (1015/1032)\n","Test Accuracy of     3: 98% (990/1010)\n","Test Accuracy of     4: 98% (966/982)\n","Test Accuracy of     5: 98% (877/892)\n","Test Accuracy of     6: 98% (943/958)\n","Test Accuracy of     7: 98% (1010/1028)\n","Test Accuracy of     8: 97% (947/974)\n","Test Accuracy of     9: 97% (988/1009)\n","\n","Test Accuracy (Overall): 98% (9835/10000)\n"],"name":"stdout"}]},{"metadata":{"id":"Zoyro_933nZ9","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}