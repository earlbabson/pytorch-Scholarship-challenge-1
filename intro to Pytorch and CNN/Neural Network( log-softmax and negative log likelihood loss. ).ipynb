{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Neural Network( log-softmax and negative log likelihood loss. ).ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"H4A0OqOUEonV","colab_type":"text"},"cell_type":"markdown","source":[" Build a model that returns the log-softmax as the output and calculate the loss using the negative log likelihood loss. Note that for nn.LogSoftmax and F.log_softmax you'll need to set the dim keyword argument appropriately. dim=0 calculates softmax across the rows, so each column sums to 1, while dim=1 calculates across the columns so each row sums to 1. Think about what you want the output to be and choose dim appropriately."]},{"metadata":{"id":"XdxwjvJJERPU","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms\n","\n","# Define a transform to normalize the data\n","transform = transforms.Compose([transforms.ToTensor(),\n","                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","                              ])\n","# Download and load the training data\n","trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JO9OxrKhFBR3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"2f8adbd5-731c-4ea3-f797-4724d8ccb30b","executionInfo":{"status":"ok","timestamp":1543018975435,"user_tz":480,"elapsed":1731,"user":{"displayName":"Ritul Patidar","photoUrl":"https://lh4.googleusercontent.com/-FN-NHp32ujY/AAAAAAAAAAI/AAAAAAAAAGg/NYELu2XQ7Lk/s64/photo.jpg","userId":"11865105037108873448"}}},"cell_type":"code","source":["# define model\n","model  = nn.Sequential( nn.Linear(784, 128),\n","                        nn.ReLU(),\n","                      nn.Linear(128, 64),\n","                      nn.ReLU(),\n","                      nn.Linear(64, 10),\n","                       nn.LogSoftmax(dim=1)\n","                     )\n","# Define the loss\n","criterion = nn.NLLLoss()\n","\n","# Get our data\n","images, labels = next(iter(trainloader))\n","# Flatten images\n","images = images.view(images.shape[0], -1)\n","\n","# Forward pass, get our logits\n","logits = model(images)\n","# Calculate the loss with the logits and the labels\n","loss = criterion(logits, labels)\n","\n","print(loss)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["tensor(2.3120, grad_fn=<NllLossBackward>)\n"],"name":"stdout"}]},{"metadata":{"id":"sdHMSdBfNqi5","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}